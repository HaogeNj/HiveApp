1.关于 unix_timestamp
目前hive的bug unix_timestamp在分区表中会走全表扫描
建议使用 to_unix_timestamp

2.关于字符集
Hadoop和hive都是用UTF-8编码的，所以中文必须是UTF-8编码，才能正常使用。
备注：中文数据load到表里面，如果字符集不同，很可能全是乱码，需要做转码，但hive本身没有函数做这个。

3.Hadoop计算框架的特性
|--数据量大不是问题，数据倾斜是问题。
|--job数比较多的作业运行效率相对比较低，比如及时有几百行的表，如果多次关联多次汇总，产生十几个jobs，耗时很长。原因是map reduce作业初始化的时间是比较长的。
|--sum，count,max,min等udaf，不怕数据倾斜的问题，hadoop在map端的汇总和并优化，使数据倾斜不是问题。
|--count(distinct)在数据量大的情况下，效率地下，如果是多个count(distinct)效率更低，因为count(distinct)是按groupby字段分组，按distinct字段排序，一般这种分布式很倾斜的，比如UV。

4.优化的常用手段
★好的模型设计事半功倍
★解决数据倾斜的问题。
★减少job数。
★设置合理的map reduce的task数，能有效提升性能。比如(10w级别的计算，用160个reduce，想当浪费，一个足够)
★了解数据分布，自己动手解决数据倾斜问题是个不错的选择。
	set hive.groupby.skewindata=true;这是通用的算法优化，但算法优化有时不能使用特定业务场景，开发人员了解业务，了解数据，可以通过业务逻辑精确有效的解决数据倾斜问题。
★数据量较大的情况下，慎用count(distinct),容易产生倾斜问题。
★对小文件进行和并，是行之有效的提高调度效率的方法，假如所有的作业设置合理的文件数，对调度效率尝试积极的影响。
★把握整体，单个作业的优化不如整体最优。

5.全排序
hive的排序关键字sort by，它有意区别于传统数据库的order by也是为了强调两者的区别--sort by只能在单机范围内排序。

6.关于压缩参数
hive.exec.compress.output这个参数，默认是false，很多时候貌似要单独显式地设置一遍
否则会对结果做压缩的，如果你的这个文件后面还要在hadoop下直接操作，就不能压缩。
